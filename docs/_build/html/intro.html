

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Project Report &mdash; DE3-Audio v1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to DE3-Audio’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> DE3-Audio
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Project Report</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#initial-planning">Initial Planning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#project-aims">Project Aims</a></li>
<li class="toctree-l3"><a class="reference internal" href="#team-coordination">Team Coordination</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-generation">Data Generation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#d-tune-in">3D Tune-In</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-generation-with-maxsp">Data Generation with MaxSP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-generation-with-python">Data Generation with Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#real-data-generation">Real Data Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-types">Data Types</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-resolution-and-length">Data Resolution and Length</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sound-type">Sound Type</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#convolutional-neural-network">Convolutional Neural Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#improvements">Improvements</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#filtering">Filtering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#final-outcome">Final Outcome</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">DE3-Audio</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Project Report</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/intro.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="project-report">
<h1>Project Report<a class="headerlink" href="#project-report" title="Permalink to this headline">¶</a></h1>
<p><em>“The music is not in the notes, but in the silence between.”</em></p>
<p><em>- Wolfgang Amadeus Mozart</em></p>
<p>This report outlines work done for the 2019 Audio Experience Design Installation
at Imperial College London. My contribution to this installation involved:</p>
<ul class="simple">
<li>Generating a mixed synthetic and real 3D audio dataset with over <a class="reference external" href="https://www.dropbox.com/sh/g511lxn3aminor6/AAA7dM8wifHG5ejbL_bbTKA_a?dl=0">2000 audio clips</a></li>
<li>Training a CNN to make heading predictions based on 2 channel audio vectors</li>
<li>Utilising a probabilistic filter to smooth heading predictions</li>
<li>Creating an interactive display, with real time audio input, and graphic output</li>
</ul>
<p>The final result of work was: an interactive audio localisation system, which utilised
live binaural recordings to make predictions on sound source location, and then project
the predicted heading on the floor.</p>
<p><strong>Dummy Head with two DPA Lapel Mics</strong></p>
<div class="figure align-center">
<img alt="_images/dummy_head_side.jpg" src="_images/dummy_head_side.jpg" />
</div>
<div class="section" id="initial-planning">
<h2>Initial Planning<a class="headerlink" href="#initial-planning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="project-aims">
<h3>Project Aims<a class="headerlink" href="#project-aims" title="Permalink to this headline">¶</a></h3>
<p>The initial aims for project were submitted in the <a class="reference external" href="https://www.dropbox.com/s/s0ut74x6u8ri9yr/AXP-TeamPingLight.docx?dl=0">preliminary document</a>:</p>
<ul class="simple">
<li><em>Model Human Audio Localisation</em></li>
</ul>
<p>Fulfilling this criteria was the bulk of the installation work. Ultimately, we were successful
in this endeavour, however, in a simpler case. Humans have the ability to differentiate
between sounds source coming from: left, right, front or back. Our system could predict left or right with 76% accuracy (see figure below).</p>
<p><strong>Test Accuracy of Algorithm</strong></p>
<div class="figure align-center">
<img alt="_images/test_acc.png" src="_images/test_acc.png" />
</div>
<ul class="simple">
<li><em>Create an Interactive Dark room, where participants can be localised in real time</em></li>
</ul>
<p>The initial vision was to locate sounds of human foot steps in a dark room, and then shine a
a spot light on their location. Early on in the project, it was suggested that utilising
a projector would be simpler then making a custom spotlight. We incorporated this suggestion, and
through testing found that the projector was bright enough to be seen, even in a lit room.</p>
<ul class="simple">
<li><em>Tie in with educational aspect to explain how our human audio localisation works</em></li>
</ul>
<p>During the Audio Experience day and Dyson Open House we spoke with visitors and explained our installation.</p>
<p>We began our explanation by asking the visitor to close their eyes, and then locate us as we moved around.
The idea was to first illustrate the incredible computation our brains do: localising
sounds in a 3D environment based purely on two auditory signals.</p>
<p>We then explained the process through which this computation is believed to be done (Duplex theory), utilising
inter-aural time difference (ITD) and inter-aural level difference (ILD). We then demonstrated
our installation which modelled this system computationally.</p>
<p>Inevitably, audience members would walk around the head and see poor performance between front and back localisation.</p>
<p>This would then lead to the discussion of cone of confusion, and we would point out that
the ITD and ILD between directly in front and back positions are identical. We also took this opportunity to explain
direction dependent filtering done by our ear pinna’s and the role of head movement.</p>
</div>
<div class="section" id="team-coordination">
<h3>Team Coordination<a class="headerlink" href="#team-coordination" title="Permalink to this headline">¶</a></h3>
<p>Team coordination was facilitated by: Messenger, <a class="reference external" href="https://www.dropbox.com/s/s0ut74x6u8ri9yr/AXP-TeamPingLight.docx?dl=0">Github</a>, Trello</p>
<p><strong>Trello Board for Task Assignment</strong></p>
<div class="figure align-center">
<img alt="_images/trello.png" src="_images/trello.png" />
</div>
</div>
</div>
<div class="section" id="data-generation">
<h2>Data Generation<a class="headerlink" href="#data-generation" title="Permalink to this headline">¶</a></h2>
<p>The field of <em>machine listening</em> is hot. Previous work has utilised large microphone arrays (5+) with custom algorithms and hand picked features [1].
Recent advances in machine learning, however, have made it possible to learn extremely complex functions from data.
These advances are being applied to reach state of the art performance in sound localisation [1].</p>
<p>In line with previous work, our team aimed to train a convolutional neural network (CNN) to predict sound location based on time series audio data. Where we hoped innovate and learn
was in the method of generating training data and in utilising only two microphones, mimicking the human system, as suppose to a large microphone array.</p>
<div class="section" id="d-tune-in">
<h3>3D Tune-In<a class="headerlink" href="#d-tune-in" title="Permalink to this headline">¶</a></h3>
<p>3D Tune-In is an open-source library for real-time binaural spatialisation [2]. Given a mono audio file, it can generate the
corresponding localised stereo recording for a point in space relative to the listener. While this mapping is complex,
for our purposes we assumed it to be a black box. We were interested only in approximating the inverse function.
Given a binaural recording, predict the location of the sound relative to the listener. The algorithm for the approximation would be a CNN.</p>
</div>
<div class="section" id="data-generation-with-maxsp">
<h3>Data Generation with MaxSP<a class="headerlink" href="#data-generation-with-maxsp" title="Permalink to this headline">¶</a></h3>
<p>In order to train the CNN, we needed a large dataset with audio clips and corresponding location labels. Rather then generate this
by hand using the offline recording feature in the 3D Tune-In test app, we accomplished this programmatically.</p>
<p>First, I set a 10 min timer and started an online recording in 3D Tune-In. A script in MaxSP, which interfaced with 3D Tune-In using open sound control (OSC),
uniformly iterate through various distance and headings, and moved the sound source. As the recording ran, the max patch would write the sound source’s current
location into a text file (<a class="reference external" href="https://github.com/zacharyyamaoka/DE3-Audio/blob/master/data_label/data_rec001.txt">example .txt file</a>).</p>
<p><strong>Moving Sound Source using MaxSP</strong></p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/EC2ePor7Wz0" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>After 10 mins, the online recording and the max patch were stopped. To utilise the data, a function was written to clip the front and end of the audio data, to
ensure it matched with the labels.</p>
</div>
<div class="section" id="data-generation-with-python">
<h3>Data Generation with Python<a class="headerlink" href="#data-generation-with-python" title="Permalink to this headline">¶</a></h3>
<p>In order to boost performance, we wanted to make sure that our training data was as close as possible to the test data. I realised we could still interface
using OSC but utilise python to create a more natural motion pattern. The natural motion meant that for a given window of data, the source would stay around the same location,
as suppose to teleporting around the sound scape. Data was recorded in the same manner as described above, but now the sound source was moved
by simulating a random polar walker. This random walker walks in circles around the listener (similar to how we imagined people would interact with the dummy head) with
speeds and accelerations similar to the average human [3].</p>
<ul class="simple">
<li>Average walking speed: 1.4 m/s</li>
<li>Average walking acceleration over short period of time: 0.86 m/s^2</li>
</ul>
<p>At each time step, there is a small probability, the walker switches directions.</p>
<p>See code for walker:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>

    <span class="c1">#update speed and orientation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">timer</span> <span class="o">+=</span> <span class="n">dt</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">r_dot</span> <span class="o">+=</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">acc_std</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">theta_dot</span> <span class="o">+=</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">acc_std</span><span class="p">)</span> <span class="c1">#in small steps....</span>

    <span class="c1">#Move person</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_dot</span> <span class="o">*</span> <span class="n">dt</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_dot</span> <span class="o">*</span> <span class="n">dt</span>

    <span class="c1"># with small probabality switch direction</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">timer</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span> <span class="c1"># every one second you may switchh</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1">#with small probability stop, mabye also fixes this unbounded increase problem</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_dot</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">r_dot</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<p>See walker in action:</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/z80D9Xikr2k" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="real-data-generation">
<h3>Real Data Generation<a class="headerlink" href="#real-data-generation" title="Permalink to this headline">¶</a></h3>
<p>The best data is data taken from the actually test distribution. To generate this dataset, I set up the dummy head and projector as it would be setup on the installation day. I then created
a display that would point to a random heading and let a python script run that captured a sound recording every 30s. Essentially: the computer would tell the person where to stand, the person
would move to that location while making sound, then the computer would capture a sound recording.</p>
<p><strong>Screen which Pointed in Direction to Stand</strong></p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/8DLFwBuzAxI" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>Following the Arrow Around the Head While Making Noise</strong></p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/zB7vwWIljaw" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>The great advantage in this approach was that data contained features specific to the dummy head we would use in the installation. ITD, IDL and especially the HRTF are greatly affected by the shape of the head,
body and the ears. The most realistic dataset we could have generated in 3D Tune-In would have utilised the publicly available Kemar HRTF coupled with using the real Kemar in the installation.
Instead we generated a fair amount of synthetic data using the incorrect HRTF, and then fine tuned our model using a large amount of real data recorded on the actually head.</p>
</div>
</div>
<div class="section" id="data-types">
<h2>Data Types<a class="headerlink" href="#data-types" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-resolution-and-length">
<h3>Data Resolution and Length<a class="headerlink" href="#data-resolution-and-length" title="Permalink to this headline">¶</a></h3>
<p>Thought was given to: what sampling frequency and bit depth should be used for recording the sound, and what the prediction window
duration should be.</p>
<p>For synthetic data, 44100 Hz and 16 bit depth was used to capture recordings in 3D Tune-In. For input sounds, Audacity was used to convert Youtube wav files, to the correct sampling frequency and mono track format required by
the toolkit.</p>
<p>For real data, we initially also used 44100 Hz and 16 bit depth. From testing, it seemed the level resolution seemed was sufficiently fine to determine ILD, but ITD would become more apparent if we increased sampling frequency.
Thus, also conscious of memory space and realtime requirements, we opted for a 96000 Hz sampling rate. This simply required changing a few parameters in our code and adjusting the sampling frequency on the MOTU Ultralight we
where using to interface with the DPA lapel mics.</p>
<p><strong>Audio Clips - 480 samples at 96000 Hz</strong></p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/o-H32zXB1Ms" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>We tested a number of different window lengths for prediction but ultimately utilised a 0.005 second window. At 96000 Hz this corresponded to 480 samples. This choice was made based on the size of the ITD feature we
wanted to capture. Based on the Woodworth’s formula for ITD, we knew that the maximum delay on our dummy head would be around 0.0006 seconds (assuming the sound travels at 340 m/s) [3].
As our CNN was not integrating information over time, it needed sufficient temporal information to make the correct decision in the moment. Too small, and the important relative information of the sound pressure
wave would be loss. Too large, and the delay features would be obscured. 0.005 seconds seemed right.</p>
<p><strong>Head Parameters for Wood Worth Formula [4]</strong></p>
<div class="figure align-center">
<img alt="_images/woodworth.png" src="_images/woodworth.png" />
</div>
</div>
<div class="section" id="sound-type">
<h3>Sound Type<a class="headerlink" href="#sound-type" title="Permalink to this headline">¶</a></h3>
<p>Thought was given to what type of sound to use in the data generation. Initially ideas that guided our thinking were:</p>
<ul class="simple">
<li><em>Used the same sound.</em> The model would require less capacity to localise one sound as suppose to learning to detect the features on many different types of sound.</li>
<li><em>Use constant dB sound</em>. If the sound level is kept the same, then the model could learn to predict distance</li>
</ul>
<p>With this in mind we decided upon a rain sound. Rain is an extremely rich signal, and we thought that there was an interesting psychological aspect as humans perceive rain to be all around us, but the
computer program would be indifferent.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/PiHM4WdmQ4o" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>It became apparent that this data had to many frequency components (essentially white noise) and was seemingly random. We felt it would be easier to learn to extract ITD and ILD features
on a simpler wave form. First clapping was tried, we hopped the algorithm would pick up on the clear time and level differences in the impulse peak.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/cxy7wylUFVw" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Then Beethoven’s Moonlight Sonata. Compared to rain, Piano sound is relatively pure, consisting mostly of a few main harmonics and their over tones.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/wGWhmaOE9mM" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Training on the piano music also meant the algorithm would better generate to other “pure tones”, like a constant whistle.</p>
</div>
</div>
<div class="section" id="convolutional-neural-network">
<h2>Convolutional Neural Network<a class="headerlink" href="#convolutional-neural-network" title="Permalink to this headline">¶</a></h2>
<p>Once the data had been collected, the CNN could be trained. First we over fit on a small amount of data to validate the model. Then training was
done using the full dataset. Interestingly we had to start with a high learning rate because the model starts in a local minimum. By initialising the weights with
with small random numbers meant, the initial prediction for any audio single would be a small random number (around 0 deg). This is a good starting point, but learning to predict
+90 or -90 deg would actually minimise overall error. This is because the model cannot differentiate between front and back (cone of confusion).</p>
<p><strong>Training the CNN with Audio Data</strong></p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/F0cH7pZOYvQ" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="improvements">
<h2>Improvements<a class="headerlink" href="#improvements" title="Permalink to this headline">¶</a></h2>
<p>We had a number of improvements to improve our audio localisation algorithm’s performance.</p>
<p>1. We created a data set using a pure sinusoid at 1.6 kHz with background noise. While this would make it impossible to detect direction dependent features, it would be simpler to for the algorithm to
extract ITD and ILD. The background noise would also make the prediction more robust in real settings.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/ThffOQjV17k" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<ol class="arabic" start="2">
<li><p class="first">We normalised and mean centred the data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">audio</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:(</span><span class="n">start</span><span class="o">+</span><span class="n">chunk</span><span class="p">)]</span>

<span class="c1">#center data</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
<span class="n">audio</span> <span class="o">-=</span> <span class="n">mean</span>

<span class="c1">#normalize</span>

<span class="nb">max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">audio</span><span class="p">))</span>
<span class="n">audio</span> <span class="o">/=</span> <span class="nb">max</span>
</pre></div>
</div>
</li>
</ol>
<p>While this removed distance information, it gave improved robustness to level differences and background noise (like that found in the installation)</p>
<p>3. We changed the localisation task from regression to classification problem. Previously our CNN was trained to predict source heading on a continuous range between 0 and 2 pi. Now it would simply predict left or
right.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/dCLHqfuBEFc" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>4. Added head movement. While previously mentioned changes lowered our angular resolution, this decrease could be offset by adding head movement. Slight head movement is a
technique used by humans to differentiate between front and back sound sources. In implementation, our dummy head was moved by a 5v servo motor powered by an Arduino Uno.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/NIZqMI7LmdQ" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<ol class="arabic simple" start="5">
<li>Added a probabilistic filter. In order to utilise head movement information, predictions needed to be integrated over time. For this, a discrete Bayes filter is utilised.</li>
</ol>
<div class="section" id="filtering">
<h3>Filtering<a class="headerlink" href="#filtering" title="Permalink to this headline">¶</a></h3>
<p>Initially, filtering of the predictions was done using a simple moving average filter:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">last_theta_mu</span><span class="p">,</span> <span class="n">last_theta_var</span><span class="p">):</span>

      <span class="c1"># simple moving average filter.</span>
      <span class="n">last_theta_mu</span> <span class="o">=</span> <span class="n">last_theta_mu</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="c1">#modulo</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">theta_mu</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pointer</span><span class="p">]</span> <span class="o">=</span> <span class="n">last_theta_mu</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">theta_var</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pointer</span><span class="p">]</span> <span class="o">=</span> <span class="n">last_theta_var</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointer</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="c1">#add wrap around</span>

      <span class="n">curr_theta_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_mu</span><span class="p">)</span>
      <span class="n">curr_theta_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_var</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">curr_theta_mu</span><span class="p">,</span> <span class="n">curr_theta_var</span>
</pre></div>
</div>
<p>In order to achieve more consistent performance, however, it became clear that a more powerful filter would be needed.
The final algorithm used a discrete Bayes filter which is more robust to spurious predictions and can
integrate predictions over time to account for head movement.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/eWNau435xrc" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>We model the sound source as a random particle that experiences a small gaussian drift each time step. The prediction is also modelled using a gaussian with variance 180 deg, to
reflect the fact the head cannot differentiate front from back.</p>
<p>Find relevant code in file <a class="reference external" href="https://github.com/zacharyyamaoka/DE3-Audio/blob/master/algo/filter.py">filter.py</a></p>
<p>Now representing our prediction as a belief between 0 and 2 pi, we felt it would be more accurate to change our display from the single slice showed in the first installation.
For the Open House, a MaxSP patch was created which wrapped belief distribution around a circle.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/Itsho3N23gU" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="section" id="final-outcome">
<h2>Final Outcome<a class="headerlink" href="#final-outcome" title="Permalink to this headline">¶</a></h2>
<p><strong>Initial Set Up</strong></p>
<div class="figure align-center">
<img alt="_images/v1_head.jpg" src="_images/v1_head.jpg" />
</div>
<p><strong>Audio Experience Day</strong></p>
<blockquote>
<div><div class="figure align-center">
<img alt="_images/v2_head.jpg" src="_images/v2_head.jpg" />
</div>
</div></blockquote>
<p><strong>Dyson Open House</strong></p>
<div class="figure align-center">
<img alt="_images/v3_head.jpg" src="_images/v3_head.jpg" />
</div>
<p><strong>Live Binaural Localization</strong></p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="//www.youtube.com/embed/GGU_w7pQqGI" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>[1] Vera-Diaz, Juan Manuel, et al.
“Towards End-to-End Acoustic Localization Using Deep Learning: From Audio Signal to Source Position Coordinates.”
2018, doi:10.20944/preprints201807.0570.v1.</p>
<p>[2] Cuevas-Rodríguez M, Picinali L, González-Toledo D, et al., 2019,
3D Tune-In Toolkit: An open-source library for real-time binaural spatialisation,
Plos One, Vol:14, Pages:e0211899-e0211899</p>
<p>[3] Lawrence, Peter.
“What Is the Maximum Walking Acceleration/Deceleration over a Very Short Time Period (E.g., 0.02, 0.1, 0.5 Sec)?”
ResearchGate, 8 Aug. 2016, www.researchgate.net/post/What_is_the_maximum_walking_acceleration_deceleration_over_a_very_short_time_period_eg_002_01_05_sec.</p>
<p>[4] Cohen, Michael. (2010). Under-explored dimensions in spatial sound. 10.1145/1900179.1900199.</p>
<p>´</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to DE3-Audio’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Zachary Yamaoka, Haroon Shams, Sophie Owen

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>